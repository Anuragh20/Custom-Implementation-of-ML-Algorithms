{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_Implementation_of_TFIDF Vectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvLvmewlxLix"
      },
      "source": [
        "<font face='georgia'>\n",
        "    \n",
        "   <h4><strong>What does tf-idf mean?</strong></h4>\n",
        "\n",
        "   <p>    \n",
        "Tf-idf stands for <em>term frequency-inverse document frequency</em>, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
        "</p>\n",
        "    \n",
        "   <p>\n",
        "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
        "</p>\n",
        "    \n",
        "   <p>\n",
        "Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
        "</p>\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XLaGFUMxLiy"
      },
      "source": [
        "<font face='georgia'>\n",
        "    <h4><strong>How to Compute:</strong></h4>\n",
        "\n",
        "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        " <ul>\n",
        "    <li>\n",
        "<strong>TF:</strong> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: <br>\n",
        "\n",
        "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}.$\n",
        "</li>\n",
        "<li>\n",
        "<strong>IDF:</strong> Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: <br>\n",
        "\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}}.$\n",
        "for numerical stabiltiy we will be changing this formula little bit\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}+1}.$\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV82tg1xLi0"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUsYm9wjxLi1"
      },
      "source": [
        "## SkLearn# Collection of string documents\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwmFZfKxLi4"
      },
      "source": [
        "### SkLearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4dfQOkxLi4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Om8YpYxLi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f125df36-20ce-4e3d-e29b-3e2aeb6ccf79"
      },
      "source": [
        "# sklearn feature names, they are sorted in alphabetic order by default.\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTKplK96xLi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae84430-790e-42c4-aea6-885082cf2451"
      },
      "source": [
        "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
        "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
        "\n",
        "print(vectorizer.idf_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTiWHygxLjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7553fe-e282-44b2-f2af-fb4aa87cada9"
      },
      "source": [
        "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
        "skl_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMG7AdDalCxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd86a1b5-191f-4fd4-f69c-0acbb16f710a"
      },
      "source": [
        "print(skl_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n",
            "  (1, 8)\t0.281088674033753\n",
            "  (1, 6)\t0.281088674033753\n",
            "  (1, 5)\t0.5386476208856763\n",
            "  (1, 3)\t0.281088674033753\n",
            "  (1, 1)\t0.6876235979836938\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (3, 8)\t0.38408524091481483\n",
            "  (3, 6)\t0.38408524091481483\n",
            "  (3, 3)\t0.38408524091481483\n",
            "  (3, 2)\t0.5802858236844359\n",
            "  (3, 1)\t0.46979138557992045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDKEpbA-xLjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523d4132-742c-4a2e-bf1a-6685fea3fe79"
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# Here the output is a sparse matrix\n",
        "print(skl_output[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QWo34hexLjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1ba76e-d930-4b11-b18b-074548a41af8"
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
        "print(skl_output[0].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIwx5LzxLjI"
      },
      "source": [
        "### Custom Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjuCcJwXxLjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a029496b-41e3-473e-fa72-72c07492b507"
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "#Fitting the data which is the equivalent of .fit() of the TFIDvecotorizer function of the Sckikit learn Library\n",
        "#Creating a list of lists with strings.Converting the sentences into its component words\n",
        "\n",
        "\n",
        "def f_c(corpus_n):                                                              #Custom Fit function to get the unique values or the Vocab \n",
        "  w=[]                                                                          #Initializing an empty list to get a list of all strings \n",
        "  for i in corpus_n:                                                            #Looping through each of the documents \n",
        "    w.append(i.split())                                                         #Splitting each of the documents into a list of strings\n",
        "  w1=[]                                                                         #Initializing new lists to get all strings together\n",
        "  u=[]                                                                            \n",
        "  #Getting the Unique words in the given corpus\n",
        "  s= \" \".join(corpus_n)                                                         #joining all the documents together and form a single string\n",
        "  w1= s.split()                                                                 #splitting the string into individual words\n",
        "  u= np.unique(w1)                                                              #Getting the unique strings in the corpus or creating a Vocab\n",
        "  return u,w                                                                    #Returning the Vocab\n",
        "\n",
        "def transfo_c(v,w2,corpus_t):                                                   #Custom Transform function to calculate the TFIDF values and return a sparse matrix of TFIDF\n",
        "  i_f=[]                                                                        #Initializing new lists to get the IDF values for each word in the vocab\n",
        "  for word in v:                                                                #looping though each of the elements of the Vocab\n",
        "    i_count=0                                                                   #Initializing a count variable to capture the frequency of each of the elements of the vocab\n",
        "    for f in w2:                                                                #looping through each of the documents to check if any of the vocab words are present in the documents\n",
        "      check= False                                                              #Setting the Check parameter to False \n",
        "      for s in f:                                                               #Looping through individual words of the document\n",
        "        if word== s:                                                            #Checking if the word is present in the document\n",
        "          check= True                                                           #Changing the check parameter to True if the vocab word is present in the document\n",
        "      if check== True:                                                          #using the if statement to see if the check parameter has been changed which confirms that the vocab word is present in the document\n",
        "        i_count= i_count+1                                                      #Increasing the count value when the vocab word is present in the document\n",
        "    i_f.append(1+(math.log((1+len(corpus_t))/(1+i_count))))                     #Computing the IDF value for each of the Vocab words\n",
        "  idf_dict= dict(zip(v,i_f))                                                    #Moving the IDF values with their corresponding vocab words into a dictionary\n",
        "  y=[]                                                                          #Initializing an empty list\n",
        "  for n in w2:                                                                  #Looping through individual documents\n",
        "    k=[]                                                                        #initializing a new list and a new dictionary\n",
        "    t_dict= {} \n",
        "    t_dict= dict(Counter(n))                                                    #Getting the frequencies of each of the individual strings\n",
        "    for a in v:                                                                 #looping though the vocab words for Computing the TF values for all the elements of a document\n",
        "      if a in n:                                                                #checking if each of the individual vocab words is present in each of the documents\n",
        "        t_count= t_dict.get(a)                                                  #Getting the frequency of each of the words of a dictionary\n",
        "      else:                                                                     \n",
        "        t_count=0                                                               #Setting the counter to zero if the vaocab word is not present in the document\n",
        "      k.append((t_count/len(n))*(idf_dict.get(a)))                              #Computing the TFIDF value for each of the words in the each of the documents\n",
        "    y.append(k)                                                                 #appeding the TFIDF values into a list\n",
        "\n",
        "  r= normalize(csr_matrix(np.array(y)), norm='l2', axis=1)                      #converting the TFIDF value into a sparse matrix and normalizing the sparse matrix\n",
        "  return r, idf_dict                                                            #Returning the TFIDF sparse matrix and the IDF values\n",
        "\n",
        "r1, r2= f_c(corpus)                                                             #Fitting the corpus data \n",
        "\n",
        "fin_res_r, fin_res_id= transfo_c(r1,r2,corpus)                                  #Transforming the fitted data and printing the output\n",
        "print(fin_res_r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n",
            "  (1, 1)\t0.6876235979836937\n",
            "  (1, 3)\t0.2810886740337529\n",
            "  (1, 5)\t0.5386476208856762\n",
            "  (1, 6)\t0.2810886740337529\n",
            "  (1, 8)\t0.2810886740337529\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.4697913855799205\n",
            "  (3, 2)\t0.580285823684436\n",
            "  (3, 3)\t0.3840852409148149\n",
            "  (3, 6)\t0.3840852409148149\n",
            "  (3, 8)\t0.3840852409148149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51j_OtqAxLjL"
      },
      "source": [
        "<font face='georgia'>\n",
        "    <h4><strong>2. Implementation of max features functionality:</strong></h4>\n",
        "\n",
        "<ul>\n",
        "    <li> As a part of this task we modify the fit and transform functions so that vocab will contain only 50 terms with top idf scores.</li>\n",
        "    <br>\n",
        "    <li>Here a pickle file, with file name <strong>cleaned_strings</strong>. will load the corpus and use it as input to the tfidf vectorizer.</li>\n",
        "    <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHxPLlwNxLjL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "42c7de41-1e14-424d-dab3-ce1f1551d719"
      },
      "source": [
        "# Below is the code to load the cleaned_strings pickle file provided\n",
        "# Here corpus is of list type\n",
        "\n",
        "import pickle\n",
        "with open('cleaned_strings', 'rb') as f:\n",
        "    corpus_2 = pickle.load(f)\n",
        "    \n",
        "# printing the length of the corpus loaded\n",
        "print(\"Number of documents in corpus = \",len(corpus_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents in corpus =  746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_OuPSWsF4qE",
        "outputId": "fe37db5d-79e8-4dc8-de28-be7c193686a1"
      },
      "source": [
        "print(corpus_2[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['slow moving aimless movie distressed drifting young man', 'not sure lost flat characters audience nearly half walked', 'attempting artiness black white clever camera angles movie disappointed became even ridiculous acting poor plot lines almost non existent', 'little music anything speak', 'best scene movie gerardo trying find song keeps running head', 'rest movie lacks art charm meaning emptiness works guess empty', 'wasted two hours', 'saw movie today thought good effort good messages kids', 'bit predictable', 'loved casting jimmy buffet science teacher']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZULfoOIdxLjQ"
      },
      "source": [
        "# Write your code here.\n",
        "# Try not to hardcode any values.\n",
        "# Make sure its well documented and readble with appropriate comments."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVTuC5DmMlH3"
      },
      "source": [
        "Fitting the new corpus using the Fit() function from Task 1 and getting the Vocab value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_DJnnR3xLjR",
        "outputId": "9a72b000-8ce0-4413-a8ad-31345e0d6ddc"
      },
      "source": [
        "q2_r1, q2_r2= f_c(corpus_2)                                                     #Fitting the new corpus data\n",
        "print(q2_r1)                                                                    #Printing to see a sample of the fitted data\n",
        "print(len(q2_r1))                                                               #getting the length of the fittted data or the Voacab or unique values in the corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aailiyah' 'abandoned' 'ability' ... 'zillion' 'zombie' 'zombiez']\n",
            "2897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyenbDICNPQ0"
      },
      "source": [
        "Taking the IDF values of all the Vocab words by using the Transform function and taking only the IDF values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mmHAMpVF4qX",
        "outputId": "bdf7f2b2-b67b-4901-a357-2c00d1c63a05"
      },
      "source": [
        "vals2, idf_val= transfo_c(q2_r1, q2_r2,corpus_2)                         #Transforming the fittted values and getting the IDF values\n",
        "print(type(idf_val))                                                     #Printing the data type of the fitted values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X-y7JRiNfUb"
      },
      "source": [
        "Getting the lenght of the IDF values list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtTFX8N2F4qb",
        "outputId": "bf837356-ee89-44da-99b8-6f8aafd06284"
      },
      "source": [
        "len(idf_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2897"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXRqfje5NkUl"
      },
      "source": [
        "Sorting the Vocab words based on the IDF values in descending order. and picking the top 50 IDF values and their corresponding words and creating a new Vocab titled \"new'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNiVUYmoF4qh"
      },
      "source": [
        "new= {}                                                                         #Creating an empty dictionary to capture the top50 IDF values and their corresponding Vocab words\n",
        "sor_idf= sorted(idf_val.items(), key=lambda x: x[1], reverse=True)              #Sorting the Vocab words based on the IDF value\n",
        "new= sor_idf[:50]                                                               #Creating a new vocab by taking the top50 vocab words based on the IDF values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia_SM_GLF4qq",
        "outputId": "04900b43-654b-4ccd-c139-7b95f189aa91"
      },
      "source": [
        "u50=[]                                                                          #Initializing an empty list to capture all the new vocab words\n",
        "iv50=[]                                                                         #initializing another empty list to capture all the IDF values\n",
        "for h in range(len(new)):                                                       #looping through the length of the new vocab\n",
        "    u50.append(new[h][0])                                                       #Capturing all the vocab words from the new Vocab\n",
        "    iv50.append(new[h][1])                                                      #Capturing the respective IDF's of each of teh new Vocab words\n",
        "\n",
        "print(u50)\n",
        "print(iv50) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aailiyah', 'abandoned', 'abroad', 'abstruse', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'achille', 'ackerman', 'actions', 'adams', 'add', 'added', 'admins', 'admiration', 'admitted', 'adrift', 'adventure', 'aesthetically', 'affected', 'affleck', 'afternoon', 'aged', 'ages', 'agree', 'agreed', 'aimless', 'aired', 'akasha', 'akin', 'alert', 'alike', 'allison', 'allow', 'allowing', 'alongside', 'amateurish', 'amaze', 'amazed', 'amazingly', 'amusing', 'amust', 'anatomist', 'angel', 'angela', 'angelina']\n",
            "[6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm34mN6uPd3-"
      },
      "source": [
        "Using the above created transform function to get the TFIDF value sparse matrix and printing the sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwBeffEjF4qw",
        "outputId": "7ff99ead-47ab-42ef-f8bc-3faa2f40fc86"
      },
      "source": [
        "res2, res2_id= transfo_c(u50,q2_r2,corpus_2)                                    #Calling the Transform function and appying the same on the new corpus and the new vocab\n",
        "print(res2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 30)\t1.0\n",
            "  (68, 24)\t1.0\n",
            "  (72, 29)\t1.0\n",
            "  (74, 31)\t1.0\n",
            "  (119, 33)\t1.0\n",
            "  (135, 3)\t0.37796447300922725\n",
            "  (135, 10)\t0.37796447300922725\n",
            "  (135, 18)\t0.37796447300922725\n",
            "  (135, 20)\t0.37796447300922725\n",
            "  (135, 36)\t0.37796447300922725\n",
            "  (135, 40)\t0.37796447300922725\n",
            "  (135, 41)\t0.37796447300922725\n",
            "  (176, 49)\t1.0\n",
            "  (181, 13)\t1.0\n",
            "  (192, 21)\t1.0\n",
            "  (193, 23)\t1.0\n",
            "  (216, 2)\t1.0\n",
            "  (222, 47)\t1.0\n",
            "  (225, 19)\t1.0\n",
            "  (227, 17)\t1.0\n",
            "  (241, 44)\t1.0\n",
            "  (270, 1)\t1.0\n",
            "  (290, 25)\t1.0\n",
            "  (333, 26)\t1.0\n",
            "  (334, 15)\t1.0\n",
            "  (341, 43)\t1.0\n",
            "  (344, 42)\t1.0\n",
            "  (348, 8)\t1.0\n",
            "  (377, 37)\t1.0\n",
            "  (409, 5)\t1.0\n",
            "  (430, 39)\t1.0\n",
            "  (457, 45)\t1.0\n",
            "  (461, 4)\t1.0\n",
            "  (465, 38)\t1.0\n",
            "  (475, 35)\t1.0\n",
            "  (493, 6)\t1.0\n",
            "  (500, 48)\t1.0\n",
            "  (548, 0)\t0.7071067811865475\n",
            "  (548, 32)\t0.7071067811865475\n",
            "  (608, 14)\t1.0\n",
            "  (612, 11)\t1.0\n",
            "  (620, 46)\t1.0\n",
            "  (632, 7)\t1.0\n",
            "  (644, 12)\t0.7071067811865475\n",
            "  (644, 27)\t0.7071067811865475\n",
            "  (664, 28)\t1.0\n",
            "  (667, 22)\t1.0\n",
            "  (691, 34)\t1.0\n",
            "  (697, 9)\t1.0\n",
            "  (722, 16)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk9JyKYdPvTr"
      },
      "source": [
        "Converting the sparse matrix into a dense matrix and checking the shape of the resultant dense matrix. The resultant dense matrix has 50 columns and the same number of document as the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQdyEWjVF4qz",
        "outputId": "7f189f5b-5318-40cb-9258-40b2d68534d2"
      },
      "source": [
        "F=res2.todense()\n",
        "F.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(746, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}